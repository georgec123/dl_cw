{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "from tensorflow.random import set_seed\n",
    "import random\n",
    "\n",
    "seed(1)\n",
    "set_seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, InputLayer\n",
    "from keras.callbacks import History \n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, ParameterGrid\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "from utils import k_fold, predict_and_accuracy, get_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "- Standardise numerical cols \n",
    "- Create new features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/Data_A.csv', header=None)\n",
    "\n",
    "columns=['midprice_up','ss_lob_1_p','ss_lob_1_v','bs_lob_1_p','bs_lob_1_v','ss_lob_2_p','ss_lob_2_v',\n",
    "'bs_lob_2_p','bs_lob_2_v','ss_lob_3_p','ss_lob_3_v','bs_lob_3_p','bs_lob_3_v','ss_lob_4_p','ss_lob_4_v',\n",
    "'bs_lob_4_p','bs_lob_4_v','c1','c2','c3','c4','c5']\n",
    "\n",
    "df.columns = columns\n",
    "\n",
    "price_cols = ['ss_lob_1_p','bs_lob_1_p','ss_lob_2_p','bs_lob_2_p','ss_lob_3_p','bs_lob_3_p','ss_lob_4_p','bs_lob_4_p']\n",
    "vol_cols = ['ss_lob_1_v','bs_lob_1_v','ss_lob_2_v','bs_lob_2_v','ss_lob_3_v','bs_lob_3_v','ss_lob_4_v','bs_lob_4_v']\n",
    "prev_mid_m_cols = ['c1', 'c2', 'c3', 'c4', 'c5']\n",
    "\n",
    "\n",
    "# engineer features\n",
    "df, engineered_cols = get_features(df)\n",
    "\n",
    "\n",
    "# do train test split\n",
    "X = df.drop(columns='midprice_up')\n",
    "y = df['midprice_up']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "means = X_train[price_cols+vol_cols+engineered_cols].mean(axis=0)\n",
    "stds = X_train[price_cols+vol_cols+engineered_cols].std(axis=0)\n",
    "\n",
    "def transform(df, means, stds):\n",
    "    return (df-means)/stds\n",
    "\n",
    "X_train[price_cols+vol_cols+engineered_cols] = transform(X_train[price_cols+vol_cols+engineered_cols], means, stds)\n",
    "X_test[price_cols+vol_cols+engineered_cols] = transform(X_test[price_cols+vol_cols+engineered_cols], means, stds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_model():\n",
    "    model = keras.Sequential([\n",
    "        InputLayer(input_shape=(len(X_train.columns))),\n",
    "        Dense(units=100, activation='elu'),\n",
    "        Dense(units=50, activation='elu'),\n",
    "        Dense(units=20, activation='elu'),\n",
    "        Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    history = History()\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs=5\n",
      "train: 0.7634, test: 0.7625\n",
      "epochs=10\n",
      "train: 0.7683, test: 0.7665\n",
      "epochs=20\n",
      "train: 0.7718, test: 0.7683\n"
     ]
    }
   ],
   "source": [
    "for epochs in [5, 10,20]:\n",
    "    print(f\"{epochs=}\")\n",
    "    \n",
    "    model, history = get_default_model()\n",
    "    \n",
    "    model.compile(optimizer=Adam(), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    model.fit(X_train, y_train, batch_size=100, epochs=epochs, callbacks=[history], verbose=0)\n",
    "    train_a, test_a = predict_and_accuracy(model,  X_train, X_test, y_train, y_test)\n",
    "    print(f\"train: {train_a:.4f}, test: {test_a:.4f}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam\n",
      "train: 0.7665, test: 0.7650\n",
      "SGD\n",
      "train: 0.7571, test: 0.7599\n"
     ]
    }
   ],
   "source": [
    "for optimizer in [Adam(), SGD()]:\n",
    "\n",
    "    print(f\"{optimizer.get_config()['name']}\")\n",
    "    \n",
    "    model, history = get_default_model()\n",
    "    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    model.fit(X_train, y_train, batch_size=100, epochs=10, callbacks=[history], verbose=0)\n",
    "    train_a, test_a = predict_and_accuracy(model,  X_train, X_test, y_train, y_test)\n",
    "    print(f\"train: {train_a:.4f}, test: {test_a:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size=50\n",
      "train: 0.7700, test: 0.7661\n",
      "\n",
      "batch_size=100\n",
      "train: 0.7677, test: 0.7640\n",
      "\n",
      "batch_size=500\n",
      "train: 0.7624, test: 0.7621\n",
      "\n",
      "batch_size=1000\n",
      "train: 0.7606, test: 0.7593\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch_size in [50, 100, 500, 1000]:\n",
    "    print(f\"{batch_size=}\")\n",
    "\n",
    "    model, history = get_default_model()\n",
    "\n",
    "    model.compile(optimizer=Adam(), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    model.fit(X_train, y_train, batch_size=batch_size, epochs=10, callbacks=[history], verbose=0)\n",
    "    train_a, test_a = predict_and_accuracy(model,  X_train, X_test, y_train, y_test)\n",
    "    print(f\"train: {train_a:.4f}, test: {test_a:.4f}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune Eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eta=0.001\n",
      "train: 0.7683, test: 0.7662\n",
      "\n",
      "eta=0.003\n",
      "train: 0.7696, test: 0.7673\n",
      "\n",
      "eta=0.01\n",
      "train: 0.7643, test: 0.7651\n",
      "\n",
      "eta=0.05\n",
      "train: 0.5028, test: 0.5022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for eta in [0.001, 0.003, 0.01, 0.05]:\n",
    "    print(f\"{eta=}\")\n",
    "\n",
    "    model, history = get_default_model()\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=eta), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    model.fit(X_train, y_train, batch_size=100, epochs=10, callbacks=[history], verbose=0)\n",
    "    train_a, test_a = predict_and_accuracy(model,  X_train, X_test, y_train, y_test)\n",
    "    print(f\"train: {train_a:.4f}, test: {test_a:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "units=50\n",
      "train: 0.7587, test: 0.7587\n",
      "\n",
      "units=100\n",
      "train: 0.7586, test: 0.7572\n",
      "\n",
      "units=200\n",
      "train: 0.7581, test: 0.7564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for units in [50, 100, 200]:\n",
    "    print(f\"{units=}\")\n",
    "\n",
    "    history = History()\n",
    "    model = keras.Sequential([\n",
    "        InputLayer(input_shape=(len(X_train.columns))),\n",
    "        Dense(units=units, activation='elu'),\n",
    "        Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    model = keras.models.clone_model(model)\n",
    "\n",
    "    eta = 0.003\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=eta), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # k_fold(model, X_train, y_train, batch_size=100, epochs=5, history=history, splits=5)\n",
    "    model.fit(X_train, y_train, batch_size=100, epochs=5, callbacks=[history], verbose=0)\n",
    "    train_a, test_a = predict_and_accuracy(model,  X_train, X_test, y_train, y_test)\n",
    "    print(f\"train: {train_a:.4f}, test: {test_a:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5a4d575a3f9390417db93755ce5d901ca6db577e6a37fb25263f9010c483d1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
